{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install camelot-py\n",
    "# !pip install opencv-python\n",
    "# !pip install ghostscript\n",
    "# !pip install pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#pdf = pdfplumber.open(\"/Users/angshuman.chakraborty@cognitedata.com/Desktop/work/BP/DDRs_samples_BP_GoM/2017/83_782_16-Daily_Ops-Partners-Report_83-(07-10-2017).pdf\")\n",
    "\n",
    "col_list = [\"From - To\\nOp. Depth (usft)\", \"Hrs\\n(hr)\",\"Phase\",'Task', 'Activity', 'Code', 'NPT', 'Critical \\nPath', 'Operation']\n",
    "def columns_check(df):\n",
    "    ll = []\n",
    "    for a in df.columns:\n",
    "        if a.startswith(col_list[0]) | a.startswith(col_list[1]) | a.startswith(col_list[2]) | a.startswith(col_list[3]) | a.startswith(col_list[4]) | a.startswith(col_list[5]) | a.startswith(col_list[6]) | a.startswith(col_list[7]) | a.startswith(col_list[8]):\n",
    "            ll.append(a)\n",
    "    return df[ll]\n",
    "\n",
    "\n",
    "def list_dir(root):\n",
    "    result = []\n",
    "    \n",
    "    for path, dirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            if name.lower().endswith((\".pdf\")):\n",
    "                result.append(os.path.join(path, name))\n",
    "                \n",
    "    return result\n",
    "\n",
    "\n",
    "def check_df_1(DF):\n",
    "    for j in range(DF.shape[0]):\n",
    "        if '- 00:00' in DF.iloc[j,0]:\n",
    "            index = j\n",
    "            DF = DF.iloc[:index+1,:]\n",
    "            break\n",
    "    return DF\n",
    "\n",
    "def check_df_2(DF):\n",
    "    for j in range(DF.shape[0]):\n",
    "        if 'Mud Log Information' in DF.iloc[j,0] or \"06:00 Update\" in DF.iloc[j,0]:\n",
    "            index = j\n",
    "            DF = DF.iloc[:index,:]\n",
    "            break\n",
    "    return DF\n",
    "\n",
    "def row_check(DF):\n",
    "    lst = []\n",
    "    for i in range(len(DF)):\n",
    "        if \"Operations Summary\" in DF.iloc[i,0] or \"From\" in DF.iloc[i,0]:\n",
    "            lst.append(i)\n",
    "\n",
    "    DF.reset_index(inplace=True)\n",
    "    DF.drop(columns = \"index\",axis=1,inplace=True)\n",
    "    # ind = row_check(all)\n",
    "    DF.drop(lst,inplace=True)  \n",
    "    return DF\n",
    "\n",
    "def get_df(pdf):\n",
    "    updated_df_i = pd.DataFrame()\n",
    "    all_df = pd.DataFrame()\n",
    "    for i in range(len(pdf.pages)):\n",
    "        #print(i)\n",
    "        page = pdf.pages[i]\n",
    "        lst = page.extract_table()\n",
    "        df_i = pd.DataFrame(lst, columns=lst[0])\n",
    "        df_i = df_i.fillna(value=\"\")\n",
    "        #df_i = df_i.dropna(axis=0, how='any')\n",
    "        \n",
    "        for k in range(df_i.shape[0]):\n",
    "            if \"Operations Summary\" in df_i.iloc[k,0]:\n",
    "                ind = k\n",
    "\n",
    "                updated_df_i = df_i.iloc[ind+1:,:]\n",
    "                updated_df_i = check_df_1(updated_df_i)\n",
    "                updated_df_i = check_df_2(updated_df_i)\n",
    "\n",
    "\n",
    "                updated_df_i = updated_df_i.dropna(axis=1)\n",
    "                headers = updated_df_i.iloc[0]\n",
    "                updated_df_i  = pd.DataFrame(updated_df_i.values[1:], columns=headers)\n",
    "                updated_df_i = columns_check(updated_df_i)\n",
    "                \n",
    "            \n",
    "                all_df = all_df.append(updated_df_i)\n",
    "                all_df = all_df.reset_index()\n",
    "                all_df.drop(columns = \"index\",axis=1,inplace=True)\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    return all_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One major function to append all DFs\n",
    "\n",
    "def get_all_data(root):\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    for path in list_dir(root):\n",
    "        print(path)\n",
    "        pdf = pdfplumber.open(path)\n",
    "        #all_tables = read_pdf(path,pages=\"all\")\n",
    "        operations_summary = get_df(pdf)\n",
    "        final_df = pd.concat([final_df,operations_summary])\n",
    "        print(final_df.shape)\n",
    "\n",
    "    final_df = row_check(final_df)\n",
    "\n",
    "    return final_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = get_all_data(\"/Users/angshuman.chakraborty@cognitedata.com/Desktop/work/BP/DDRs_samples_BP_GoM/2017\") ## folder path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing\n",
    "<b>Operations summary: Cleaning and Preparation</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"/Users/angshuman.chakraborty@cognitedata.com/Desktop/work/BP/DDRs/all2021.csv\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = DF.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "    df.replace({'\\n' : ''}, regex = True, inplace = True)\n",
    "    df.columns = [col[:19] if '(ft)' in col else col for col in df.columns]\n",
    "    df.columns = [col[:19] if '(usft)' in col else col for col in df.columns]\n",
    "    df = df.rename(columns = {'From - To\\nOp. Depth' : 'From - To Op. Depth',\n",
    "                            'Hrs\\n(hr)' : 'Hrs (hr)',\n",
    "                            'Critical \\nPath' : 'Critical Path'})\n",
    "    \n",
    "\n",
    "    df['From - To Op. Depth'] = df['From - To Op. Depth'].astype('str')\n",
    "    df['From - To'] = [value[:13] for value in df['From - To Op. Depth']]\n",
    "    df['Depth (usft)'] = [value[14:] for value in df['From - To Op. Depth']]\n",
    "    df.drop(['From - To Op. Depth'], axis = 1, inplace = True)\n",
    "    \n",
    "    df = df.fillna(' ')\n",
    "    order = [8,9,0,1,2,3,4,5,6,7] # setting column's order\n",
    "    new_df = df[[df.columns[i] for i in order]]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes and merges white spaces and splits rows into multiple rows based on activity\n",
    "def removeSpaces(df):\n",
    "    columns = [col for col in df.columns if col != 'Operation' and col != 'From - To']\n",
    "    indices = []\n",
    "    for i in range(len(df)):\n",
    "        j = 0\n",
    "        for col in columns:\n",
    "            if df.loc[i, col] == ' ':\n",
    "                j += 1\n",
    "        if j == len(columns) - 1:\n",
    "            indices.append(i)\n",
    "    for index in indices:\n",
    "        df.loc[index - 1, 'Operation'] = df.loc[index - 1, 'Operation'] + ' ' + df.loc[index, 'Operation']\n",
    "    df = df.drop(indices)\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    df = df.assign(Operation=df.Operation.str.split('. -')).explode('Operation') ## this could be document/well specific - needs to be modified probably\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from html import unescape\n",
    "\n",
    "\n",
    "def processing_sentence(df):\n",
    "\n",
    "    # Remove Punctuations\n",
    "    punc_str = '!#$%&\\*+,-:;<=>?@[\\\\]^_`{|}~'\n",
    "    df['processed_operation'] = df['Operation'].apply(lambda x: \"\".\\\n",
    "                                                            join([char for char in x if char not in punc_str]))\n",
    "\n",
    "    # # Lower case all the words: Not necessary?\n",
    "    # df['processed_operation'] = df['Operation'].apply(lambda x : x.lower())\n",
    "    # df['processed_operation'] = df['Operation'].str.replace(\"[\\d-]\",'',regex=True)\n",
    "\n",
    "    # Removal of stopwords\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    def remove_stopwords(text):\n",
    "        output= [i for i in text if i not in stopwords]\n",
    "        return output\n",
    "\n",
    "    df['processed_operation'] = df['processed_operation'].apply(lambda x:remove_stopwords(x))\n",
    "    df['processed_operation'] = df['processed_operation'].apply(lambda x: \"\".join(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(df)\n",
    "df = removeSpaces(df)\n",
    "df = processing_sentence(df)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26e9f1aa537e6430ec6c6d83cbc562bb94dd7eeb52229bedeb59b9c01e3652fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
